{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code_12_HAN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SQoEzO-1eML0","colab_type":"code","outputId":"8e88218c-a13d-43d8-fecd-3bfc61fceba5","executionInfo":{"status":"ok","timestamp":1564433457636,"user_tz":-120,"elapsed":9593,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\n","import random\n","\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import roc_auc_score, average_precision_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from keras.preprocessing import text, sequence\n","from keras.layers import Embedding, SpatialDropout1D\n","from keras.models import Model, Sequential\n","from keras.layers import Dense, Embedding, Input\n","from keras.optimizers import RMSprop\n","import keras.backend as K\n","import keras\n","from keras.layers import Dense, Input, GRU, LSTM, Bidirectional, Dropout, CuDNNLSTM, CuDNNGRU, GlobalAveragePooling1D, GlobalMaxPool1D, TimeDistributed\n","from keras.optimizers import RMSprop, Adam\n","from keras.regularizers import l2\n","from keras.layers import BatchNormalization, SpatialDropout1D, LSTM\n","from sklearn.metrics import average_precision_score, roc_auc_score\n","from sklearn.model_selection import StratifiedKFold\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.engine.topology import Layer, InputSpec\n","from keras import initializers as initializers, regularizers, constraints\n","\n","from numpy.random import seed\n","from tensorflow import set_random_seed\n","import random as rn\n","import os\n","from nltk import tokenize \n","import nltk\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"qCeSyw_eCrx4","colab_type":"code","outputId":"840ba010-6187-4455-9cd4-7c640f4e2aa8","executionInfo":{"status":"ok","timestamp":1564433491332,"user_tz":-120,"elapsed":43277,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# GOOGLE COLAB SETUP\n","\n","# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nak1uhxiEK0R","colab_type":"code","colab":{}},"source":["#2. Get the file\n","data_path          = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/data'\n","codes_path         = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/codes'\n","cv_models_path     = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/cv_models'\n","models_path        = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/models'\n","\n","\n","#3. Read file as panda dataframe\n","train         = pd.read_csv(f'{data_path}/train_cleaned_sent_punkt.csv') \n","test_labelled = pd.read_csv(f'{data_path}/test_labelled_cleaned_sent_punkt.csv') \n","test_unlabelled = pd.read_csv(f'{data_path}/test_unlabelled_cleaned_sent_punkt.csv') "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f3ZeAcM_eGpj","colab_type":"code","colab":{}},"source":["train['mal']         = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1  \n","train.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n","train.comment_text.fillna(\"empty\", inplace=True)\n","\n","test_labelled['mal'] = test_labelled[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1  \n","test_labelled.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n","test_labelled.comment_text.fillna(\"empty\", inplace=True)\n","\n","test_unlabelled.comment_text.fillna(\"empty\", inplace=True)\n","\n","# CHANGE TRAIN AND TEST, MIX TO GET SIMILAR DISTRIBUTION\n","from sklearn.model_selection import train_test_split\n","rs=42\n","X_train1, X_test1, y_train1, y_test1  = train_test_split(train.drop('mal', axis=1), train.mal, stratify=train.mal, test_size=0.29, random_state=rs )\n","X_train2, X_test2, y_train2, y_test2  = train_test_split(test_labelled.drop('mal', axis=1), test_labelled.mal, stratify=test_labelled.mal, test_size=0.29, random_state=rs)\n","\n","X = np.concatenate((X_train1.comment_text, X_train2.comment_text))\n","y = np.concatenate((y_train1, y_train2))\n","\n","X_test = np.concatenate((X_test1.comment_text, X_test2.comment_text))\n","y_test = np.concatenate((y_test1, y_test2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWPSaXneFFZu","colab_type":"code","colab":{}},"source":["#X = train.comment_text\n","#y = train.mal\n","\n","max_features = 40000\n","maxlen       = 400\n","dropout_rate = 0.25\n","rs           = 42\n","epochs       = 4\n","batch_size   = 256\n","embed_dim    = 50\n","rec_units    = 150\n","\n","\n","max_sen_len = 100\n","max_sent_amount = 4\n","\n","seed(rs)\n","set_random_seed(rs)\n","rn.seed(rs)\n","\n","os.environ['PYTHONHASHSEED']=str(rs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DPm1-btWNGB","colab_type":"code","colab":{}},"source":["def dot_product(x, kernel):\n","    if K.backend() == 'tensorflow':\n","        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n","    else:\n","        return K.dot(x, kernel)\n","\n","class AttentionWithContext(Layer):\n","    def __init__(self,\n","                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, u_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","\n","        self.supports_masking = True\n","        self.init = initializers.get('glorot_uniform')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.u_regularizer = regularizers.get(u_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.u_constraint = constraints.get(u_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        super(AttentionWithContext, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight((input_shape[-1], input_shape[-1]),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        if self.bias:\n","            self.b = self.add_weight((input_shape[-1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","\n","        self.u = self.add_weight((input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_u'.format(self.name),\n","                                 regularizer=self.u_regularizer,\n","                                 constraint=self.u_constraint)\n","\n","        super(AttentionWithContext, self).build(input_shape)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        return None\n","\n","    def call(self, x, mask=None):\n","        uit = dot_product(x, self.W)\n","\n","        if self.bias:\n","            uit += self.b\n","\n","        uit = K.tanh(uit)\n","        ait = dot_product(uit, self.u)\n","\n","        a = K.exp(ait)\n","\n","        if mask is not None:\n","            # Cast the mask to floatX to avoid float64 upcasting in theano\n","            a *= K.cast(mask, K.floatx())\n","\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], input_shape[-1]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ac0ThFO3FIHI","colab_type":"code","colab":{}},"source":["def make_hat(MAX_SENT_LENGTH = max_sen_len, MAX_SENTS=max_sent_amount, MAX_NB_WORDS=max_features, EMBEDDING_DIM=embed_dim, rec_units=rec_units):\n","\n","    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n","    embedded_sequences = Embedding(MAX_NB_WORDS+1, EMBEDDING_DIM, trainable=True)(sentence_input)\n","    embedded_sequences = SpatialDropout1D(dropout_rate)(embedded_sequences)\n","    l_lstm = Bidirectional(CuDNNGRU(rec_units, return_sequences=True))(embedded_sequences)\n","    l_att = AttentionWithContext()(l_lstm)\n","    sentEncoder = Model(sentence_input, l_att)\n","\n","    review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n","    review_encoder = TimeDistributed(sentEncoder)(review_input)\n","    l_lstm_sent = Bidirectional(CuDNNGRU(rec_units, return_sequences=True))(review_encoder)\n","    l_att_sent = AttentionWithContext()(l_lstm_sent)\n","\n","    preds = Dense(1, activation='sigmoid')(l_att_sent)\n","    model = Model(review_input, preds)\n","\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer=RMSprop(clipvalue=1, clipnorm=1),\n","                  metrics=['acc'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BFgVXjNPF-dM","colab_type":"code","colab":{}},"source":["kf = StratifiedKFold(n_splits=5, random_state=rs)\n","auc = []\n","roc = []\n","c = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XkbO2fFvBXhC","colab_type":"code","colab":{}},"source":["def clean_str(string):\n","    #string = string.replace(\",\", \".\").replace(\";\", \".\").replace(\":\", \".\").replace(\"-\", \".\")\n","    return string.strip().lower()\n","\n","def tok_sentence(s):\n","    temp = tokenizer.texts_to_sequences(s)\n","    if len(temp)==0:\n","        return np.array([0])\n","    return temp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-49hvW4_GHww","colab_type":"code","outputId":"73993c88-f550-401d-ee1b-7a794b796fd1","executionInfo":{"status":"ok","timestamp":1564435342360,"user_tz":-120,"elapsed":1744544,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for c, (train_index, val_index) in enumerate(kf.split(X, y)):\n","    print(f' fold {c}')\n","    X_train, X_val       = X[train_index], X[val_index]\n","    y_train, y_val       = y[train_index], y[val_index] \n","    \n","    \n","    train_posts = []\n","    train_labels = []\n","    train_texts = []\n","    \n","    tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_features)\n","    tokenizer.fit_on_texts(X_train)\n","    \n","    #TRAIN\n","    for i, value in enumerate(X_train):\n","        if(i%10000==0):\n","            print(i)\n","        text = clean_str(value)\n","        train_texts.append(text)\n","        sentences = tokenize.sent_tokenize(text)\n","        sentences = tok_sentence(sentences)\n","        x = len(sentences)<max_sent_amount\n","        while x:\n","            sentences.append(np.array([0])) \n","            x = len(sentences)<max_sent_amount\n","\n","        if len(sentences)>max_sent_amount:\n","            sentences = sentences[0:max_sent_amount]\n","        sentences = sequence.pad_sequences(sentences, maxlen=max_sen_len)\n","\n","        train_posts.append(sentences)\n","    \n","    val_posts = []\n","    val_labels = []\n","    val_texts = []\n","\n","    #VAL\n","    for i, value in enumerate(X_val):\n","        if(i%10000==0):\n","            print(i)\n","        text = clean_str(value)\n","        val_texts.append(text)\n","        sentences = tokenize.sent_tokenize(text)\n","        sentences = tok_sentence(sentences)\n","\n","\n","        x = len(sentences)<max_sent_amount\n","        while x:\n","            sentences.append(np.array([0])) \n","            x = len(sentences)<max_sent_amount\n","\n","        if len(sentences)>max_sent_amount:\n","            sentences = sentences[0:max_sent_amount]\n","        sentences = sequence.pad_sequences(sentences, maxlen=max_sen_len)\n","        val_posts.append(sentences)\n","    \n","    X_train = np.array(train_posts)\n","    y_train = np.array(y_train)\n","    X_val =  np.array(val_posts)\n","    y_val = np.array(y_val)\n","    \n","    del train_posts\n","    del val_posts\n","    \n","    \n","    model                = make_hat()\n","    print('Fitting')\n","    history              = model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_val, y_val), epochs=4, shuffle=True, verbose=1)\n","    probs                = model.predict(X_val, batch_size=batch_size, verbose=1)\n","    \n","    model.save_weights(f'{cv_models_path}/HAN_fold_{c}.h5')\n","    \n","    auc_f                = average_precision_score(y_val, probs)\n","    auc.append(auc_f)\n","    roc_f                = roc_auc_score(y_val, probs)\n","    roc.append(roc_f)\n","    print(f' average precision {round(auc_f,3)}')\n","    print(f' roc auc {round(roc_f,3)}')\n","    del model\n","    K.clear_session()"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" fold 0\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","110000\n","120000\n","0\n","10000\n","20000\n","30000\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0729 20:54:12.872534 139624020133760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0729 20:54:12.873977 139624020133760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0729 20:54:12.882664 139624020133760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0729 20:54:12.903214 139624020133760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0729 20:54:12.912525 139624020133760 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0729 20:54:15.366203 139624020133760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0729 20:54:15.372953 139624020133760 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","W0729 20:54:15.380795 139624020133760 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Fitting\n","Train on 126974 samples, validate on 31745 samples\n","Epoch 1/4\n","126974/126974 [==============================] - 75s 594us/step - loss: 0.1832 - acc: 0.9373 - val_loss: 0.1204 - val_acc: 0.9579\n","Epoch 2/4\n","126974/126974 [==============================] - 72s 568us/step - loss: 0.1265 - acc: 0.9526 - val_loss: 0.1156 - val_acc: 0.9597\n","Epoch 3/4\n","126974/126974 [==============================] - 72s 564us/step - loss: 0.1156 - acc: 0.9565 - val_loss: 0.1148 - val_acc: 0.9613\n","Epoch 4/4\n","126974/126974 [==============================] - 72s 564us/step - loss: 0.1088 - acc: 0.9587 - val_loss: 0.1144 - val_acc: 0.9610\n","31745/31745 [==============================] - 5s 166us/step\n"," average precision 0.871\n"," roc auc 0.966\n"," fold 1\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","110000\n","120000\n","0\n","10000\n","20000\n","30000\n","Fitting\n","Train on 126974 samples, validate on 31745 samples\n","Epoch 1/4\n","126974/126974 [==============================] - 73s 578us/step - loss: 0.1956 - acc: 0.9322 - val_loss: 0.1193 - val_acc: 0.9581\n","Epoch 2/4\n","126974/126974 [==============================] - 72s 564us/step - loss: 0.1261 - acc: 0.9530 - val_loss: 0.1175 - val_acc: 0.9603\n","Epoch 3/4\n","126974/126974 [==============================] - 72s 563us/step - loss: 0.1152 - acc: 0.9567 - val_loss: 0.1266 - val_acc: 0.9562\n","Epoch 4/4\n","126974/126974 [==============================] - 71s 562us/step - loss: 0.1075 - acc: 0.9591 - val_loss: 0.1252 - val_acc: 0.9570\n","31745/31745 [==============================] - 5s 165us/step\n"," average precision 0.865\n"," roc auc 0.966\n"," fold 2\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","110000\n","120000\n","0\n","10000\n","20000\n","30000\n","Fitting\n","Train on 126976 samples, validate on 31743 samples\n","Epoch 1/4\n","126976/126976 [==============================] - 73s 574us/step - loss: 0.1951 - acc: 0.9323 - val_loss: 0.1271 - val_acc: 0.9556\n","Epoch 2/4\n","126976/126976 [==============================] - 71s 562us/step - loss: 0.1263 - acc: 0.9529 - val_loss: 0.1200 - val_acc: 0.9598\n","Epoch 3/4\n","126976/126976 [==============================] - 71s 561us/step - loss: 0.1147 - acc: 0.9566 - val_loss: 0.1282 - val_acc: 0.9561\n","Epoch 4/4\n","126976/126976 [==============================] - 71s 561us/step - loss: 0.1072 - acc: 0.9594 - val_loss: 0.1244 - val_acc: 0.9588\n","31743/31743 [==============================] - 5s 165us/step\n"," average precision 0.865\n"," roc auc 0.964\n"," fold 3\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","110000\n","120000\n","0\n","10000\n","20000\n","30000\n","Fitting\n","Train on 126976 samples, validate on 31743 samples\n","Epoch 1/4\n","126976/126976 [==============================] - 73s 574us/step - loss: 0.1763 - acc: 0.9380 - val_loss: 0.1458 - val_acc: 0.9474\n","Epoch 2/4\n","126976/126976 [==============================] - 71s 561us/step - loss: 0.1192 - acc: 0.9563 - val_loss: 0.1383 - val_acc: 0.9472\n","Epoch 3/4\n","126976/126976 [==============================] - 71s 560us/step - loss: 0.1089 - acc: 0.9594 - val_loss: 0.1344 - val_acc: 0.9501\n","Epoch 4/4\n","126976/126976 [==============================] - 71s 561us/step - loss: 0.1014 - acc: 0.9622 - val_loss: 0.1421 - val_acc: 0.9439\n","31743/31743 [==============================] - 5s 165us/step\n"," average precision 0.809\n"," roc auc 0.959\n"," fold 4\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","110000\n","120000\n","0\n","10000\n","20000\n","30000\n","Fitting\n","Train on 126976 samples, validate on 31743 samples\n","Epoch 1/4\n","126976/126976 [==============================] - 73s 576us/step - loss: 0.1765 - acc: 0.9416 - val_loss: 0.1802 - val_acc: 0.9267\n","Epoch 2/4\n","126976/126976 [==============================] - 71s 562us/step - loss: 0.1136 - acc: 0.9590 - val_loss: 0.1606 - val_acc: 0.9345\n","Epoch 3/4\n","126976/126976 [==============================] - 71s 561us/step - loss: 0.1032 - acc: 0.9626 - val_loss: 0.1595 - val_acc: 0.9364\n","Epoch 4/4\n","126976/126976 [==============================] - 71s 563us/step - loss: 0.0962 - acc: 0.9653 - val_loss: 0.1810 - val_acc: 0.9263\n","31743/31743 [==============================] - 5s 165us/step\n"," average precision 0.784\n"," roc auc 0.961\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S51--4EBjMKx","colab_type":"code","outputId":"65cee105-acfe-4400-cef1-f0504d9732ce","executionInfo":{"status":"ok","timestamp":1564435342364,"user_tz":-120,"elapsed":1743186,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["round(np.array(auc).mean(), 3)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.839"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"EPtxc1qPTfoC","colab_type":"code","outputId":"eb78a009-1d73-4b30-ce6c-d75827a14817","executionInfo":{"status":"ok","timestamp":1564435342364,"user_tz":-120,"elapsed":1742720,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["round(np.array(roc).mean(),3)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.963"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"r0hIZHoGctiz","colab_type":"code","outputId":"af500c1b-de02-453f-81af-9a967344a274","executionInfo":{"status":"ok","timestamp":1564435756663,"user_tz":-120,"elapsed":2156274,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":578}},"source":["X_train   = X\n","y_train   = y\n","tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_features, oov_token='unknown')\n","tokenizer.fit_on_texts(X_train)\n","\n","train_posts = []\n","train_labels = []\n","train_texts = []\n","\n","# FULL TRAIN\n","for i, value in enumerate(X):\n","    if(i%10000==0):\n","        print(i)\n","    text = clean_str(value)\n","    train_texts.append(text)\n","    sentences = tokenize.sent_tokenize(text)\n","    sentences = tok_sentence(sentences)\n","    x = len(sentences)<max_sent_amount\n","    while x:\n","        sentences.append(np.array([0])) \n","        x = len(sentences)<max_sent_amount\n","\n","    if len(sentences)>max_sent_amount:\n","        sentences = sentences[0:max_sent_amount]\n","    sentences = sequence.pad_sequences(sentences, maxlen=max_sen_len)\n","\n","    train_posts.append(sentences)\n","\n","    \n","test_posts = []\n","test_labels = []\n","test_texts = []\n","    \n","    \n","#Test\n","for i, value in enumerate(X_test):\n","    if(i%10000==0):\n","        print(i)\n","    text = clean_str(value)\n","    test_texts.append(text)\n","    sentences = tokenize.sent_tokenize(text)\n","    sentences = tok_sentence(sentences)\n","    x = len(sentences)<max_sent_amount\n","    while x:\n","        sentences.append(np.array([0])) \n","        x = len(sentences)<max_sent_amount\n","\n","    if len(sentences)>max_sent_amount:\n","        sentences = sentences[0:max_sent_amount]\n","    sentences = sequence.pad_sequences(sentences, maxlen=max_sen_len)\n","\n","    test_posts.append(sentences)\n","    \n","    \n","X_train = np.array(train_posts)\n","y_train = np.array(y)\n","X_test =  np.array(test_posts)\n","y_test = np.array(y_test)\n","\n","del train_posts\n","del test_posts\n","\n","model   = make_hat()\n","\n","y_train = np.array(y_train)\n","y_test  = np.array(y_test)\n","\n","print('Fitting')\n","model.fit(X_train, y_train,   batch_size=batch_size, epochs=epochs, shuffle=True, verbose=1)\n","probs = model.predict(X_test, batch_size=batch_size, verbose=1)\n","auc_f = average_precision_score(y_test, probs)\n","roc_f = roc_auc_score(y_test, probs)\n","model.save_weights(f'{models_path}/HAN.h5')\n","del model"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","110000\n","120000\n","130000\n","140000\n","150000\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","Fitting\n","Epoch 1/4\n","158719/158719 [==============================] - 84s 530us/step - loss: 0.1749 - acc: 0.9405\n","Epoch 2/4\n","158719/158719 [==============================] - 83s 522us/step - loss: 0.1219 - acc: 0.9550\n","Epoch 3/4\n","158719/158719 [==============================] - 83s 522us/step - loss: 0.1123 - acc: 0.9579\n","Epoch 4/4\n","158719/158719 [==============================] - 83s 524us/step - loss: 0.1062 - acc: 0.9599\n","64830/64830 [==============================] - 11s 162us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gmjiSTh4PCSA","colab_type":"code","outputId":"0198f14e-ca28-4ae3-af41-3b1dbc56a317","executionInfo":{"status":"ok","timestamp":1564435756664,"user_tz":-120,"elapsed":2154835,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["round(auc_f,3)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.833"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"k-ptNl8KDarn","colab_type":"code","outputId":"e3bddf0f-f01a-4fbc-95cd-4520fb402c4d","executionInfo":{"status":"ok","timestamp":1564435757555,"user_tz":-120,"elapsed":2154937,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["round(roc_f,3)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.964"]},"metadata":{"tags":[]},"execution_count":16}]}]}