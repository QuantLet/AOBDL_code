{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code_1_preparation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"CwSVBuG-E6X6","colab_type":"code","colab":{}},"source":["# MODULES IMPORT \n","\n","import numpy as np\n","import pandas as pd\n","import pdb"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YB0lSfPVEpA9","colab_type":"code","outputId":"716c21e2-ca06-4c97-f249-67114ab5495f","executionInfo":{"status":"ok","timestamp":1559806663152,"user_tz":-120,"elapsed":37937,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# GOOGLE COLAB SETUP\n","\n","# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7esVSpP-E5ZD","colab_type":"code","colab":{}},"source":["# VARIABLES AND PATHS\n","\n","raw_data_path = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/raw_data'\n","data_path     = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/data'\n","codes_path    = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/codes'\n","\n","\n","#3. Read file as panda dataframe\n","train         = pd.read_csv(f'{raw_data_path}/train.csv.zip') \n","test          = pd.read_csv(f'{raw_data_path}/test.csv.zip') \n","labels        = pd.read_csv(f'{raw_data_path}/test_labels.csv.zip')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ilY_NUAI3NV","colab_type":"code","colab":{}},"source":["#Combine labels and test set\n","test                = test.merge(labels, how=\"left\", on=\"id\")\n","test['used_for_sc'] = test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 0 \n","test_labelled       = test[test.used_for_sc==True]\n","test_unlabelled     = test[test.used_for_sc!=True]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhBEDM5JLots","colab_type":"code","colab":{}},"source":["# Clean data completely \n","def substitute_repeats_fixed_len(text, nchars, ntimes=3):\n","    return re.sub(r\"(\\S{{{}}})(\\1{{{},}})\".format(nchars, ntimes-1), r\"\\1\", text)\n","  \n","def substitute_repeats(text, ntimes=3):\n","    for nchars in range(1, 20):\n","        text         = substitute_repeats_fixed_len(text, nchars, ntimes)\n","    return text\n","\n","def text_to_wordlist(text, remove_stop_words=True, stem_words=False, with_punct_sent=False):\n","    stop_words       = set(['a', 'the', \"an\", \"are\", \"as\",  'did',\n","                       \"do\", \"is\", \"has\", \"have\", \"had\", \"was\", \"were\",\n","                       \"will\", \"would\", \"am\", \"it\", \"for\", \"on\", \"it\", \"of\"])\n","    #from string import punctuation\n","    import re\n","    NEG_CONTRACTIONS    = [\n","                          (r'aren\\'t', 'are not'),\n","                          (r'can\\'t', 'can not'),\n","                          (r'couldn\\'t', 'could not'),\n","                          (r'daren\\'t', 'dare not'),\n","                          (r'didn\\'t', 'did not'),\n","                          (r'doesn\\'t', 'does not'),\n","                          (r'don\\'t', 'do not'),\n","                          (r'isn\\'t', 'is not'),\n","                          (r'hasn\\'t', 'has not'),\n","                          (r'haven\\'t', 'have not'),\n","                          (r'hadn\\'t', 'had not'),\n","                          (r'mayn\\'t', 'may not'),\n","                          (r'mightn\\'t', 'might not'),\n","                          (r'mustn\\'t', 'must not'),\n","                          (r'needn\\'t', 'need not'),\n","                          (r'oughtn\\'t', 'ought not'),\n","                          (r'shan\\'t', 'shall not'),\n","                          (r'shouldn\\'t', 'should not'),\n","                          (r'wasn\\'t', 'was not'),\n","                          (r'weren\\'t', 'were not'),\n","                          (r'won\\'t', 'will not'),\n","                          (r'wouldn\\'t', 'would not'),\n","                          (r'ain\\'t', 'am not') # not only but stopword anyway\n","                          ]\n","    OTHER_CONTRACTIONS = [\n","                          #(r\"'m\", 'am'),\n","                          (r\"'ll\", ' will'),\n","                          (r\"'s\", ' has'), # or 'is' but both are stopwords\n","                          (r\"'d\", ' had'), # or 'would' but both are stopwords\n","                          (r\"'ve\", \" have\"),\n","                           (r\"'re\", \" are\")   \n","    ]\n","    OTHER_RPS          = [\n","                          (\"&lt;3\", \" good \"),\n","                          (\":d\", \" good \"),\n","                          (\":dd\", \" good \"),\n","                          (\":p\", \" good \"),\n","                          (\"8)\", \" good \"),\n","                          (\":-)\", \" good \"),\n","                          (\":)\", \" good \"),\n","                          (\";)\", \" good \"),\n","                          (\"(-:\", \" good \"),\n","                          (\"(:\", \" good \"),\n","                          (\"yay!\", \" good \"),\n","                          (\"yay\", \" good \"),\n","                          (\"yaay\", \" good \"),\n","                          (\"yaaay\", \" good \"),\n","                          (\"yaaaay\", \" good \"),\n","                          (\"yaaaaay\", \" good \"),\n","                          (\":/\", \" bad \"),\n","                          (\":&gt;\", \" sad \"),\n","                          (\":')\", \" sad \"),\n","                          (\":-(\", \" bad \"),\n","                          (\":(\", \" bad \"),\n","                          (\":s\", \" bad \"),\n","                          (\":-s\", \" bad \"),\n","                          (\"&lt;3\", \" heart \"),\n","                          (\":d\", \" smile \"),\n","                          (\":p\", \" smile \"),\n","                          (\":dd\", \" smile \"),\n","                          (\"8)\", \" smile \"),\n","                          (\":-)\", \" smile \"),\n","                          (\":)\", \" smile \"),\n","                          (\";)\", \" smile \"),\n","                          (\"(-:\", \" smile \"),\n","                          (\"(:\", \" smile \"),\n","                          (\":/\", \" worry \"),\n","                          (\":&gt;\", \" angry \"),\n","                          (\":')\", \" sad \"),\n","                          (\":-(\", \" sad \"),\n","                          (\":(\", \" sad \"),\n","                          (\":s\", \" sad \"),\n","                          (\":-s\", \" sad \"),\n","                          (r\"\\br\\b\", \"are\"),\n","                          (r\"\\bu\\b\", \"you\"),\n","                          (r\"\\bhaha\\b\", \"ha\"),\n","                          (r\"\\bhahaha\\b\", \"ha\"),\n","                          (r\"\\bdon't\\b\", \"do not\"),\n","                          (r\"\\bdoesn't\\b\", \"does not\"),\n","                          (r\"\\bdidn't\\b\", \"did not\"),\n","                          (r\"\\bhasn't\\b\", \"has not\"),\n","                          (r\"\\bhaven't\\b\", \"have not\"),\n","                          (r\"\\bhadn't\\b\", \"had not\"),\n","                          (r\"\\bwon't\\b\", \"will not\"),\n","                          (r\"\\bwouldn't\\b\", \"would not\"),\n","                          (r\"\\bcan't\\b\", \"can not\"),\n","                          (r\"\\bcannot\\b\", \"can not\"),\n","                          (r\"\\bi'm\\b\", \"i am\"),\n","                          (\"m\", \"am\"),\n","                          (\"r\", \"are\"),\n","                          (\"u\", \"you\"),\n","                          (\"haha\", \"ha\"),\n","                          (\"hahaha\", \"ha\"),\n","                          (\"don't\", \"do not\"),\n","                          (\"doesn't\", \"does not\"),\n","                          (\"didn't\", \"did not\"),\n","                          (\"hasn't\", \"has not\"),\n","                          (\"haven't\", \"have not\"),\n","                          (\"hadn't\", \"had not\"),\n","                          (\"won't\", \"will not\"),\n","                          (\"wouldn't\", \"would not\"),\n","                          (\"can't\", \"can not\"),\n","                          (\"cannot\", \"can not\"),\n","                          (\"i'm\", \"i am\"),\n","                          (\"m\", \"am\"),\n","                          (\"i'll\" , \"i will\"),\n","                          (\"its\" , \"it is\"),\n","                          (\"it's\" , \"it is\"),\n","                          (\"'s\" , \" is\"),\n","                          (\"that's\" , \"that is\"),\n","                          (\"weren't\" , \"were not\")\n","    ]\n","    \n","    # Clean the text, with the option to remove stop_words and to stem words.\n","    text                = text.lower()\n","    \n","    for t in NEG_CONTRACTIONS:\n","            text        = re.sub(t[0], t[1], text)\n","\n","\n","    for t in OTHER_CONTRACTIONS:\n","            text        = re.sub(t[0], t[1], text)\n","    for t in OTHER_RPS:\n","            #print(t)\n","            text.replace(t[0], t[1])\n","            #text = re.sub(t[0], t[1], text)    \n","    \n","    # Clean the text\n","    if with_punct_sent:\n","      text              = re.sub(r\"[^A-Za-z0-9!(),-.:;?]\", \" \", text)\n","    else: \n","      text              = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n","    text                = re.sub(r\"what's\", \"\", text)\n","    #text               = re.sub(r\"What's\", \"\", text)\n","    text                = re.sub(r\"\\'s\", \" \", text)\n","    text                = re.sub(r\"\\'ve\", \" have \", text)\n","    text                = re.sub(r\"can\\'t\", \"cannot \", text)\n","    text                = re.sub(r\"n\\'t\", \" not \", text)\n","    text                = re.sub(r\"i\\'m\", \"i am\", text)\n","    text                = re.sub(r\" m \", \" am \", text)\n","    text                = re.sub(r\"\\'re\", \" are \", text)\n","    text                = re.sub(r\"\\'d\", \" would \", text)\n","    text                = re.sub(r\"\\'ll\", \" will \", text)\n","    text                = re.sub('[0-9]+', '', text)\n","   \n","    \n","    \n","    if with_punct_sent==False:\n","        pass\n","        #text = ''.join([c for c in text if c not in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'])\n","    else: \n","        text = ''.join([c for c in text if c not in '!.?'])\n","        \n","    # Optionally, remove stop words\n","    if remove_stop_words:\n","        text           = text.split()\n","        text           = [w for w in text if not w in stop_words]\n","        text           = \" \".join(text)\n","\n","    # Optionally, shorten words to their stems\n","    if stem_words:\n","        text           = text.split()\n","        stemmer        = SnowballStemmer('english')\n","        stemmed_words  = [stemmer.stem(word) for word in text]\n","        text           = \" \".join(stemmed_words)\n","   \n","    ltr = text.split()\n","    new_data = []\n","    for i in ltr:\n","        arr = str(i).split()\n","        xx = \"\"\n","        for j in arr:\n","            j = str(j).lower()\n","            if j[:4] == 'http' or j[:3] == 'www':\n","                continue\n","            xx += j + ' '\n","        new_data.append(xx)\n","    text = ''.join(new_data)\n","    return(text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKQkh-EkSZyn","colab_type":"code","outputId":"1b3556cb-a324-4bf5-e29a-688d665d5a4f","executionInfo":{"status":"ok","timestamp":1559806882874,"user_tz":-120,"elapsed":64434,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print('Preparing the data without any punctuation')\n","train_no_punct                            = train.copy()\n","test_labelled_no_punct                    = test_labelled.copy()\n","test_unlabelled_no_punct                  = test_unlabelled.copy()\n","\n","print('Train')\n","train_no_punct.loc[:,'comment_text']            = train['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False))\n","print('Test labelled')\n","test_labelled_no_punct.loc[:,'comment_text']    = test_labelled['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False))\n","print('Test unlabelled')\n","test_unlabelled_no_punct.loc[:,'comment_text']  = test_unlabelled['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Preparing the data without any punctuation\n","Train\n","Test labelled\n","Test unlabelled\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rikI5ZSYUltl","colab_type":"code","colab":{}},"source":["train_no_punct.to_csv(f'{data_path}/train_cleaned_no_punkt.csv', index=False)\n","test_labelled_no_punct.drop('used_for_sc', axis=1).to_csv(f'{data_path}/test_labelled_cleaned_no_punkt.csv', index=False)\n","test_unlabelled_no_punct.drop('used_for_sc', axis=1).to_csv(f'{data_path}/test_unlabelled_cleaned_no_punkt.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LoqeUel9VWOx","colab_type":"code","outputId":"94b737aa-d63f-471e-e0bb-43a40292d1d4","executionInfo":{"status":"ok","timestamp":1559806953895,"user_tz":-120,"elapsed":132086,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print('Preparing the data with sentence punctuation')\n","train_sent_punct = train.copy()\n","test_labelled_sent_punct = test_labelled.copy()\n","test_unlabelled_sent_punct = test_unlabelled.copy()\n","\n","print('Train')\n","train_sent_punct.loc[:,'comment_text']            = train['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False))\n","print('Test labelled')\n","test_labelled_sent_punct.loc[:,'comment_text']    = test_labelled['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False))\n","print('Test unlabelled')\n","test_unlabelled_sent_punct.loc[:,'comment_text']  = test_unlabelled['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Preparing the data with sentence punctuation\n","Train\n","Test labelled\n","Test unlabelled\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bGjZHk_vV7p0","colab_type":"code","colab":{}},"source":["train_sent_punct.to_csv(f'{data_path}/train_cleaned_sent_punkt.csv', index=False)\n","test_labelled_sent_punct.drop('used_for_sc', axis=1).to_csv(f'{data_path}/test_labelled_cleaned_sent_punkt.csv', index=False)\n","test_unlabelled_sent_punct.drop('used_for_sc', axis=1).to_csv(f'{data_path}/test_unlabelled_cleaned_sent_punkt.csv', index=False)"],"execution_count":0,"outputs":[]}]}