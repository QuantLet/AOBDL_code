{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code_9_HAN_improved.ipynb","version":"0.3.2","provenance":[{"file_id":"1SHtBRbJnNkBI4mjp738KPJwqy8SE_J6N","timestamp":1560279083962},{"file_id":"1JoeelGKF1j1FDdox6MpDvxYAviOUsO3M","timestamp":1535306197707}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"qCeSyw_eCrx4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"8f115d08-bc82-4899-dac1-3eb2b41f2993","executionInfo":{"status":"ok","timestamp":1560330863520,"user_tz":-120,"elapsed":2968,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}}},"source":["import numpy as np\n","import pandas as pd\n","import re \n","import os \n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from keras.models import Model, Sequential \n","from keras.layers import Dense, Embedding, Input, Flatten, Embedding, Dropout\n","from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU , SpatialDropout1D, CuDNNLSTM, TimeDistributed, CuDNNGRU\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.optimizers import RMSprop, Adam\n","from keras.engine.topology import Layer, InputSpec\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","from tensorflow import set_random_seed\n","from numpy.random import seed\n","from nltk import tokenize \n","\n","from keras.models import Model\n","from keras import backend as K\n","\n","from keras.engine.topology import Layer, InputSpec\n","from keras import initializers as initializers, regularizers, constraints\n","from keras.callbacks import Callback\n","\n","from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from keras.preprocessing.sequence import pad_sequences\n","from keras import backend as K\n","from keras.preprocessing import text\n","\n","from keras.optimizers import RMSprop, Adam\n","from keras.regularizers import l2\n","from keras.layers import BatchNormalization, SpatialDropout1D, LSTM\n","from keras.layers import maximum, GlobalAveragePooling1D\n","\n","from numpy.random import seed\n","from tensorflow import set_random_seed\n","import random as rn\n","import os"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"o3IlLjmgV_m_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":128},"outputId":"bdf3375c-04c0-4395-fded-128fb7f1c603","executionInfo":{"status":"ok","timestamp":1560330886207,"user_tz":-120,"elapsed":22777,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}}},"source":["# GOOGLE COLAB SETUP\n","\n","# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nak1uhxiEK0R","colab_type":"code","colab":{}},"source":["#2. Get the file\n","data_path     = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/data'\n","codes_path    = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/codes'\n","cv_models_path     = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/cv_models'\n","models_path        = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/models'\n","\n","#3. Read file as panda dataframe\n","train           = pd.read_csv(f'{data_path}/train_cleaned_sent_punkt.csv')\n","test_labelled   = pd.read_csv(f'{data_path}/test_labelled_cleaned_sent_punkt.csv')\n","test_unlabelled = pd.read_csv(f'{data_path}/test_unlabelled_cleaned_sent_punkt.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBfuliqNDQP3","colab_type":"code","colab":{}},"source":["train['mal']         = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1  \n","train.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n","train.comment_text.fillna(\"empty\", inplace=True)\n","\n","test_labelled['mal'] = test_labelled[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1  \n","test_labelled.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n","test_labelled.comment_text.fillna(\"empty\", inplace=True)\n","\n","test_unlabelled.comment_text.fillna(\"empty\", inplace=True)\n","\n","# CHANGE TRAIN AND TEST, MIX TO GET SIMILAR DISTRIBUTION\n","rs=42\n","X_train1, X_test1, y_train1, y_test1  = train_test_split(train.drop('mal', axis=1), train.mal, stratify=train.mal, test_size=0.29, random_state=rs )\n","X_train2, X_test2, y_train2, y_test2  = train_test_split(test_labelled.drop('mal', axis=1), test_labelled.mal, stratify=test_labelled.mal, test_size=0.29, random_state=rs)\n","\n","X = np.concatenate((X_train1.comment_text, X_train2.comment_text))\n","y = np.concatenate((y_train1, y_train2))\n","\n","X_test = np.concatenate((X_test1.comment_text, X_test2.comment_text))\n","y_test = np.concatenate((y_test1, y_test2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wT7IPTaxgIoe","colab_type":"code","colab":{}},"source":["#words_in_post = train.comment_text.apply(lambda x: len(str(x).split()))\n","#let_in_post = train.comment_text.apply(lambda x: len(str(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWPSaXneFFZu","colab_type":"code","colab":{}},"source":["max_features = 40000\n","maxlen = 400\n","dropout_rate = 0\n","rs = 42\n","epochs = 4\n","batch_size = 256\n","embed_dim = 50\n","rec_units = 150\n","\n","\n","seed(rs)\n","set_random_seed(rs)\n","rn.seed(rs)\n","\n","os.environ['PYTHONHASHSEED']=str(rs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ac0ThFO3FIHI","colab_type":"code","colab":{}},"source":["if K.backend() == 'tensorflow':\n","    import tensorflow as tf\n","\n","    config = tf.ConfigProto(intra_op_parallelism_threads=16,  \n","                            inter_op_parallelism_threads=16, \n","                            allow_soft_placement=True, \n","                            device_count = {'CPU': 16})\n","    session = tf.Session(config=config)\n","    K.set_session(session)\n","    \n","def dot_product(x, kernel):\n","    if K.backend() == 'tensorflow':\n","        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n","    else:\n","        return K.dot(x, kernel)\n","\n","class AttentionWithContext(Layer):\n","    def __init__(self,\n","                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, u_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","\n","        self.supports_masking = True\n","        self.init = initializers.get('zeros')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.u_regularizer = regularizers.get(u_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.u_constraint = constraints.get(u_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        super(AttentionWithContext, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        if self.bias:\n","            self.b = self.add_weight((input_shape[-1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","\n","        self.u = self.add_weight((input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_u'.format(self.name),\n","                                 regularizer=self.u_regularizer,\n","                                 constraint=self.u_constraint)\n","\n","        super(AttentionWithContext, self).build(input_shape)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        return None\n","\n","    def call(self, x, mask=None):\n","        uit = dot_product(x, self.W)\n","\n","        if self.bias:\n","            uit += self.b\n","\n","        uit = K.tanh(uit)\n","        ait = dot_product(uit, self.u)\n","\n","        a = K.exp(ait)\n","\n","        if mask is not None:\n","            # Cast the mask to floatX to avoid float64 upcasting in theano\n","            a *= K.cast(mask, K.floatx())\n","\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], input_shape[-1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSCzAN3KdUg6","colab_type":"code","colab":{}},"source":["from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n","class AucPrRecEvaluation(Callback):\n","    def __init__(self, validation_data=(), interval=1):\n","        super(Callback, self).__init__()\n","\n","        self.interval = interval\n","        self.X_val, self.y_val = validation_data\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if epoch % self.interval == 0:\n","            y_pred = self.model.predict(self.X_val, verbose=0)\n","            score = average_precision_score(self.y_val, y_pred)\n","            print(\"\\n AUC-Precision Recall - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BFgVXjNPF-dM","colab_type":"code","colab":{}},"source":["kf = StratifiedKFold(n_splits=5, random_state=rs)\n","auc = []\n","roc = []\n","c = 0\n","tokenizer = text.Tokenizer(num_words=max_features)\n","tokenizer.fit_on_texts(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6tvRuV9-FED","colab_type":"code","colab":{}},"source":["import re\n","sentences_dot = train.comment_text.apply(lambda x: len(x.split('.')))\n","sentences_q   = train.comment_text.apply(lambda x: len(x.split('?')))\n","sentences_ex  = train.comment_text.apply(lambda x: len(x.split('!')))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3dl-n61K_tUq","colab_type":"code","colab":{}},"source":["sentences = sentences_dot + sentences_ex + sentences_q"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rSOLqMO-ubu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":181},"outputId":"9493dd3d-436d-4159-a38d-96c1967b6ca7","executionInfo":{"status":"ok","timestamp":1560331379435,"user_tz":-120,"elapsed":590,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}}},"source":["sentences.describe()"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    159571.000000\n","mean          8.322208\n","std          27.070768\n","min           3.000000\n","25%           5.000000\n","50%           6.000000\n","75%           9.000000\n","max        4945.000000\n","Name: comment_text, dtype: float64"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"Cfc-e7lRGE3C","colab_type":"code","colab":{}},"source":["#max_sen_len = 50\n","#max_sent_amount = 25\n","max_sen_len = 40\n","max_sent_amount = 15"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PrG4VR-lOQO","colab_type":"code","outputId":"aee00505-ab9e-4ebb-b353-5023db73bbc8","executionInfo":{"status":"ok","timestamp":1560332160263,"user_tz":-120,"elapsed":1026,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["#from tensorflow.python.client import device_lib\n","\n","#print(device_lib.list_local_devices())\n","from keras.layers import GlobalAveragePooling1D\n","import nltk\n","nltk.download('punkt')"],"execution_count":39,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"DmbpRYW0zXJ0","colab_type":"code","outputId":"000cc35c-b531-4eb7-82e4-971326aa4c84","executionInfo":{"status":"error","timestamp":1560332160515,"user_tz":-120,"elapsed":895,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":389}},"source":["def clean_str(string):\n","    string = string.replace(\",\", \".\").replace(\";\", \".\").replace(\":\", \".\").replace(\"-\", \".\")\n","    return string.strip().lower()\n","\n","def tok_sentence(s):\n","    #temp = t.texts_to_sequences(s.replace(\"?\", \" \").replace(\"!\", \" \").replace(\".\", \" \").split())\n","    temp = tokenizer.texts_to_sequences(s)\n","    #print(s)\n","    if len(temp)==0:\n","        return [[0]]\n","    #else:\n","    #    temp = np.concatenate(temp).astype(int)\n","    #if type(temp[0])==list:\n","    #    temp = [item for sublist in temp for item in sublist]\n","    return temp#.reshape(-1)  \n","\n","padding = True\n","\n","train_posts = []\n","train_labels = []\n","train_texts = []\n","\n","# FULL TRAIN\n","for i, value in enumerate(X):\n","    if(i%10000==0):\n","        print(i)\n","    text = clean_str(value)\n","    #train_texts.append(text)\n","    sentences = tokenize.sent_tokenize(text)\n","    sentences = [re.sub(r\"[^A-Za-z0-9]\", \" \", i).strip() for i in sentences]\n","    sentences = [s for s in sentences if len(s.split())>1]\n","    sentences = tok_sentence(sentences)\n","    x = len(sentences)<max_sent_amount\n","    while x:\n","        sentences.append(np.array([0])) \n","        x = len(sentences)<max_sent_amount\n","    if len(sentences)>max_sent_amount:\n","        sentences = sentences[0:max_sent_amount]\n","    sentences = pad_sequences(sentences, maxlen=max_sen_len, padding='post')\n","     \n","    train_posts.append(sentences)\n","\n","    \n","test_posts = []\n","test_labels = []\n","test_texts = []\n","    \n","    \n","#Test\n","for i, value in enumerate(X_test):\n","    if(i%10000==0):\n","        print(i)\n","    text = clean_str(value)\n","    #test_texts.append(text)\n","    sentences = tokenize.sent_tokenize(text)\n","    sentences = [re.sub(r\"[^A-Za-z0-9]\", \" \", i).strip() for i in sentences]\n","    sentences = [s for s in sentences if len(s.split())>1]\n","    sentences = tok_sentence(sentences)\n","    x = len(sentences)<max_sent_amount\n","    while x:\n","        sentences.append(np.array([0])) \n","        x = len(sentences)<max_sent_amount\n","    if len(sentences)>max_sent_amount:\n","        sentences = sentences[0:max_sent_amount]\n","    sentences = pad_sequences(sentences, maxlen=max_sen_len, padding='post')\n","    test_posts.append(sentences)\n","    \n","    \n","X_train = np.array(train_posts)\n","y_train = np.array(y)\n","X_test =  np.array(test_posts)\n","y_test = np.array(y_test)\n","\n","del train_posts\n","del test_posts"],"execution_count":40,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-7be609b8afba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m#train_texts.append(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-7be609b8afba>\u001b[0m in \u001b[0;36mclean_str\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtok_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'replace'"]}]},{"cell_type":"code","metadata":{"id":"NTY9Qt_pCkUu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":908},"outputId":"3fb42c94-0d24-40b0-fb10-f2cde62d50f7","executionInfo":{"status":"ok","timestamp":1560332095011,"user_tz":-120,"elapsed":651,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}}},"source":["X_train"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[   55,     6,    73, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        ...,\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0]],\n","\n","       [[  152,     1,    13, ...,     0,     0,     0],\n","        [  526,     1,    82, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        ...,\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0]],\n","\n","       [[ 1641,     8,    13, ...,     0,     0,     0],\n","        [ 1198,     1,  7984, ...,     0,     0,     0],\n","        [   12,    43,    54, ...,     4,     0,     0],\n","        ...,\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0]],\n","\n","       ...,\n","\n","       [[   77,    49,  1302, ...,    23,  2332, 10334],\n","        [    4,     7, 13407, ...,   695,     0,     0],\n","        [  925,   495,    13, ...,     0,     0,     0],\n","        ...,\n","        [    2,    48,     5, ...,     0,     0,     0],\n","        [  997,     1,   402, ...,     0,     0,     0],\n","        [   10,  5957,     5, ...,     0,     0,     0]],\n","\n","       [[10376, 16142,   131, ...,     0,     0,     0],\n","        [    2,  3148,    41, ...,     0,     0,     0],\n","        [  123,     5,   138, ...,     4,  3148,  6539],\n","        ...,\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0]],\n","\n","       [[  108,     4,   324, ...,     0,     0,     0],\n","        [    4, 22732,    61, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        ...,\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0],\n","        [    0,     0,     0, ...,     0,     0,     0]]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"Tlof2IB4GFTx","colab_type":"code","colab":{}},"source":["def make_hat(MAX_SENT_LENGTH = max_sen_len, MAX_SENTS=max_sent_amount, MAX_NB_WORDS=max_features, EMBEDDING_DIM=embed_dim, rec_units=rec_units):\n","    \n","    sentence_input     = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n","    embedded_sequences = Embedding(MAX_NB_WORDS+1, EMBEDDING_DIM, trainable=True)(sentence_input)\n","    embedded_sequences = SpatialDropout1D(dropout_rate)(embedded_sequences)\n","    l_lstm             = Bidirectional(CuDNNGRU(rec_units, return_sequences=True, kernel_regularizer=l2(0.1)))(embedded_sequences)\n","    l_att              = AttentionWithContext()(l_lstm)\n","    sentEncoder        = Model(sentence_input, l_att)\n","\n","    review_input   = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n","    review_encoder = TimeDistributed(sentEncoder)(review_input)\n","    l_lstm_sent    = Bidirectional(CuDNNGRU(rec_units, return_sequences=True,  kernel_regularizer=l2(0.1)))(review_encoder)\n","    l_att_sent     = AttentionWithContext()(l_lstm_sent)\n","    preds          = Dense(1, activation='sigmoid')(l_att_sent)\n","    model          = Model(review_input, preds)\n","\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer=RMSprop(clipvalue=1, clipnorm=1),\n","                  metrics=['acc'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-49hvW4_GHww","colab_type":"code","outputId":"96d1fe64-3422-4af8-9bf8-5593a4dc03cd","executionInfo":{"status":"error","timestamp":1560332077590,"user_tz":-120,"elapsed":651940,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":1504}},"source":["from sklearn.metrics import average_precision_score, roc_auc_score\n","\n","X = X_train\n","y = y_train\n","\n","c = 0\n","for train_index, val_index in kf.split(X, y):\n","    print(f' fold {c}')\n","        \n","    X_train, X_val = X[train_index], X[val_index]\n","    y_train, y_val = y[train_index], y[val_index] \n","    \n","    model = make_hat()\n","    print(model.summary())\n","    \n","    PrAuc = AucPrRecEvaluation(validation_data=(X_val, y_val), interval=1)\n","    #filepath=\"weights_base.best.hdf5\"\n","    #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n","    #early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n","    callbacks_list = [PrAuc] #[checkpoint, early, PrAuc]\n","    \n","    \n","    print('Fitting')\n","    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=False,\n","              validation_data=(X_val, y_val),  verbose=1)\n","    \n","    probs = model.predict(X_val, batch_size=batch_size, verbose=1)\n","    \n","    model.save_weights(f'{cv_models_path}/HAN_fold_{c}.h5')\n","    \n","    \n","    auc_f = average_precision_score(y_val, probs)\n","    auc.append(auc_f)\n","    roc_f = roc_auc_score(y_val, probs)\n","    roc.append(roc_f)\n","    print(f' average precision {auc_f}')\n","    print(f' roc auc {roc_f}')\n","    c += 1\n","    \n","    del model"],"execution_count":36,"outputs":[{"output_type":"stream","text":[" fold 0\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 15, 25)            0         \n","_________________________________________________________________\n","time_distributed_1 (TimeDist (None, 15, 300)           2272450   \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 15, 300)           406800    \n","_________________________________________________________________\n","attention_with_context_2 (At (None, 300)               90600     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 301       \n","=================================================================\n","Total params: 2,770,151\n","Trainable params: 2,770,151\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Fitting\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Train on 126974 samples, validate on 31745 samples\n","Epoch 1/4\n","126974/126974 [==============================] - 135s 1ms/step - loss: 3.4647 - acc: 0.8995 - val_loss: 0.3373 - val_acc: 0.8995\n","Epoch 2/4\n","126974/126974 [==============================] - 131s 1ms/step - loss: 0.3351 - acc: 0.8995 - val_loss: 0.3361 - val_acc: 0.8995\n","Epoch 3/4\n","126974/126974 [==============================] - 131s 1ms/step - loss: 0.3465 - acc: 0.8980 - val_loss: 0.3360 - val_acc: 0.8995\n","Epoch 4/4\n","126974/126974 [==============================] - 131s 1ms/step - loss: 0.3347 - acc: 0.8995 - val_loss: 0.3353 - val_acc: 0.8995\n","31745/31745 [==============================] - 9s 284us/step\n"," average precision 0.08139102673245333\n"," roc auc 0.4182350003480186\n"," fold 1\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         (None, 15, 25)            0         \n","_________________________________________________________________\n","time_distributed_2 (TimeDist (None, 15, 300)           2272450   \n","_________________________________________________________________\n","bidirectional_4 (Bidirection (None, 15, 300)           406800    \n","_________________________________________________________________\n","attention_with_context_4 (At (None, 300)               90600     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 301       \n","=================================================================\n","Total params: 2,770,151\n","Trainable params: 2,770,151\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Fitting\n","Train on 126974 samples, validate on 31745 samples\n","Epoch 1/4\n","  1792/126974 [..............................] - ETA: 3:47 - loss: 72.5419 - acc: 0.9007"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-68a6c2b9a894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=False,\n\u001b[0;32m---> 25\u001b[0;31m               validation_data=(X_val, y_val),  verbose=1)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"nk3G8lMP6xxM","colab_type":"code","colab":{}},"source":["np.array(auc).mean()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mpNPlef6yki","colab_type":"code","colab":{}},"source":["np.array(roc).mean()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EPtxc1qPTfoC","colab_type":"code","colab":{}},"source":["def clean_str(string):\n","    string = string.replace(\",\", \".\").replace(\";\", \".\").replace(\":\", \".\").replace(\"-\", \".\")\n","    return string.strip().lower()\n","\n","def tok_sentence(s):\n","    #temp = t.texts_to_sequences(s.replace(\"?\", \" \").replace(\"!\", \" \").replace(\".\", \" \").split())\n","    temp = tokenizer.texts_to_sequences(s)\n","    #print(s)\n","    if len(temp)==0:\n","        return np.array([0])\n","    #else:\n","    #    temp = np.concatenate(temp).astype(int)\n","    #if type(temp[0])==list:\n","    #    temp = [item for sublist in temp for item in sublist]\n","    return temp#.reshape(-1)  \n","\n","\n","\n","train_posts = []\n","train_labels = []\n","train_texts = []\n","\n","# FULL TRAIN\n","for i, value in enumerate(X):\n","    if(i%10000==0):\n","        print(i)\n","    text = clean_str(value)\n","    train_texts.append(text)\n","    sentences = tokenize.sent_tokenize(text)\n","    sentences = tok_sentence(sentences)\n","    x = len(sentences)<max_sent_amount\n","    while x:\n","        sentences.append(np.array([0])) \n","        x = len(sentences)<max_sent_amount\n","\n","    if len(sentences)>max_sent_amount:\n","        sentences = sentences[0:max_sent_amount]\n","    sentences = pad_sequences(sentences, maxlen=max_sen_len)\n","\n","    train_posts.append(sentences)\n","\n","    \n","test_posts = []\n","test_labels = []\n","test_texts = []\n","    \n","    \n","#Test\n","for i, value in enumerate(X_test):\n","    if(i%10000==0):\n","        print(i)\n","    text = clean_str(value)\n","    test_texts.append(text)\n","    sentences = tokenize.sent_tokenize(text)\n","    sentences = tok_sentence(sentences)\n","    x = len(sentences)<max_sent_amount\n","    while x:\n","        sentences.append(np.array([0])) \n","        x = len(sentences)<max_sent_amount\n","\n","    if len(sentences)>max_sent_amount:\n","        sentences = sentences[0:max_sent_amount]\n","    sentences = pad_sequences(sentences, maxlen=max_sen_len)\n","\n","    test_posts.append(sentences)\n","    \n","    \n","X_train = np.array(train_posts)\n","y_train = np.array(y)\n","X_test =  np.array(test_posts)\n","y_test = np.array(y_test)\n","\n","del train_posts\n","del test_posts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0hIZHoGctiz","colab_type":"code","colab":{}},"source":["#X_train, y_train = X, y\n","\n","model = make_hat()\n","PrAuc = AucPrRecEvaluation(validation_data=(X_test, y_test), interval=1)\n","    #filepath=\"weights_base.best.hdf5\"\n","    #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n","    #early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n","callbacks_list = [PrAuc] #[checkpoint, early, PrAuc]\n","\n","print('Fitting')\n","model.fit(X_train, y_train, batch_size=256, epochs=epochs, shuffle=False,  validation_data=(X_test, y_test), callbacks = callbacks_list, verbose=1)\n","#probs = model.predict(X_test, batch_size=600, verbose=1)\n","from sklearn.metrics import average_precision_score, roc_auc_score\n","#auc_f = average_precision_score(y_test, probs)\n","#roc_f = roc_auc_score(y_test, probs)\n","#print(auc_f)\n","#print(roc_f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kZOICambLndP","colab_type":"code","outputId":"21e9966f-e177-47e4-8f4a-573edcebe452","executionInfo":{"status":"ok","timestamp":1536696705174,"user_tz":-120,"elapsed":2383304,"user":{"displayName":"Elizaveta","photoUrl":"//lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s50-c-k-no/photo.jpg","userId":"107472479468125275008"}},"colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["model.fit(X_train, y_train, batch_size=512, epochs=4, shuffle=False,  validation_data=(X_test, y_test), callbacks = callbacks_list, verbose=1)\n","#probs = model.predict(X_test, batch_size=batch_size, verbose=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 158719 samples, validate on 64830 samples\n","Epoch 1/4\n","158719/158719 [==============================] - 524s 3ms/step - loss: 0.1056 - acc: 0.9680 - val_loss: 0.1720 - val_acc: 0.9474\n","\n"," AUC-Precision Recall - epoch: 1 - score: 0.810497 \n","\n","Epoch 2/4\n","158719/158719 [==============================] - 524s 3ms/step - loss: 0.1018 - acc: 0.9695 - val_loss: 0.1749 - val_acc: 0.9475\n","\n"," AUC-Precision Recall - epoch: 2 - score: 0.807299 \n","\n","Epoch 3/4\n","158719/158719 [==============================] - 523s 3ms/step - loss: 0.0987 - acc: 0.9710 - val_loss: 0.1746 - val_acc: 0.9476\n","\n"," AUC-Precision Recall - epoch: 3 - score: 0.804978 \n","\n","Epoch 4/4\n","158719/158719 [==============================] - 524s 3ms/step - loss: 0.0957 - acc: 0.9725 - val_loss: 0.1714 - val_acc: 0.9480\n","\n"," AUC-Precision Recall - epoch: 4 - score: 0.801119 \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f7fd1ac3cf8>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"nxHGGtJEPOAh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}