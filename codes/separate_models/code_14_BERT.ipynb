{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code_14_BERT.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"N4erm4Xh0K2J","colab_type":"code","outputId":"2d5d0cc8-7aaa-4fc2-ed9b-c77fc5722d47","executionInfo":{"status":"ok","timestamp":1567147945737,"user_tz":-120,"elapsed":6552,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCeI0QdxlRLcqMztd5HySSXx9D_ct4tg31n5g9W=s64","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["!pip install bert-tensorflow"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\r\u001b[K     |████▉                           | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"32eUSBJK0Nue","colab_type":"code","outputId":"456853a6-c4ed-4544-d25c-18c546d7f99f","executionInfo":{"status":"ok","timestamp":1567147948256,"user_tz":-120,"elapsed":2444,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCeI0QdxlRLcqMztd5HySSXx9D_ct4tg31n5g9W=s64","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import pickle\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization"],"execution_count":2,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0830 06:52:28.277209 140180833314688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vvQ3dDRg0SNg","colab_type":"code","colab":{}},"source":["def pretty_print(result):\n","    df = pd.DataFrame([result]).T\n","    df.columns = [\"values\"]\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"03K5sVA70WQH","colab_type":"code","colab":{}},"source":["def create_tokenizer_from_hub_module(bert_model_hub):\n","  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","  with tf.Graph().as_default():\n","    bert_module = hub.Module(bert_model_hub)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    with tf.Session() as sess:\n","      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                            tokenization_info[\"do_lower_case\"]])\n","      \n","  return bert.tokenization.FullTokenizer(\n","      vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","def make_features(dataset, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN):\n","    input_example = dataset.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","    features = bert.run_classifier.convert_examples_to_features(input_example, label_list, MAX_SEQ_LENGTH, tokenizer)\n","    return features\n","\n","def create_model(bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","  \"\"\"Creates a classification model.\"\"\"\n","\n","  bert_module = hub.Module(\n","      bert_model_hub,\n","      trainable=True)\n","  bert_inputs = dict(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids)\n","  bert_outputs = bert_module(\n","      inputs=bert_inputs,\n","      signature=\"tokens\",\n","      as_dict=True)\n","\n","  # Use \"pooled_output\" for classification tasks on an entire sentence.\n","  # Use \"sequence_outputs\" for token-level output.\n","  output_layer = bert_outputs[\"pooled_output\"]\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  # Create our own layer to tune for politeness data.\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","\n","    # Dropout helps prevent overfitting\n","    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    # Convert labels into one-hot encoding\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n","    # If we're predicting, we want predicted labels and the probabiltiies.\n","    if is_predicting:\n","      return (predicted_labels, log_probs)\n","\n","    # If we're train/eval, compute loss between predicted and actual label\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","    return (loss, predicted_labels, log_probs)\n","\n","# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(bert_model_hub, num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","\n","    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","    \n","    # TRAIN and EVAL\n","    if not is_predicting:\n","\n","      (loss, predicted_labels, log_probs) = create_model(\n","        bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      train_op = bert.optimization.create_optimizer(\n","          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","      # Calculate evaluation metrics. \n","      def metric_fn(label_ids, predicted_labels):\n","        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","        f1_score = tf.contrib.metrics.f1_score(\n","            label_ids,\n","            predicted_labels)\n","        auc = tf.metrics.auc(\n","            label_ids,\n","            predicted_labels)\n","        prc = tf.metrics.auc(\n","            label_ids,\n","            predicted_labels,\n","            curve='PR',\n","            summation_method='careful_interpolation')\n","#         recall = tf.metrics.recall(\n","#             label_ids,\n","#             predicted_labels)\n","#         precision = tf.metrics.precision(\n","#             label_ids,\n","#             predicted_labels) \n","#         true_pos = tf.metrics.true_positives(\n","#             label_ids,\n","#             predicted_labels)\n","#         true_neg = tf.metrics.true_negatives(\n","#             label_ids,\n","#             predicted_labels)   \n","#         false_pos = tf.metrics.false_positives(\n","#             label_ids,\n","#             predicted_labels)  \n","#         false_neg = tf.metrics.false_negatives(\n","#             label_ids,\n","#             predicted_labels)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"f1_score\": f1_score,\n","            \"auc\": auc,\n","            \"prc\": prc\n","#             \"precision\": precision,\n","#             \"recall\": recall,\n","#             \"true_positives\": true_pos,\n","#             \"true_negatives\": true_neg,\n","#             \"false_positives\": false_pos,\n","#             \"false_negatives\": false_neg\n","        }\n","\n","      eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","          loss=loss,\n","          train_op=train_op)\n","      else:\n","          return tf.estimator.EstimatorSpec(mode=mode,\n","            loss=loss,\n","            eval_metric_ops=eval_metrics)\n","    else:\n","      (predicted_labels, log_probs) = create_model(\n","        bert_model_hub, is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      predictions = {\n","          'probabilities': log_probs,\n","          'labels': predicted_labels\n","      }\n","      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  # Return the actual model function in the closure\n","  return model_fn\n","\n","def estimator_builder(bert_model_hub, OUTPUT_DIR, SAVE_SUMMARY_STEPS, SAVE_CHECKPOINTS_STEPS, label_list, LEARNING_RATE, num_train_steps, num_warmup_steps, BATCH_SIZE):\n","\n","    # Specify outpit directory and number of checkpoint steps to save\n","    run_config = tf.estimator.RunConfig(\n","        model_dir=OUTPUT_DIR,\n","        save_summary_steps=SAVE_SUMMARY_STEPS,\n","        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n","\n","    model_fn = model_fn_builder(\n","      bert_model_hub = bert_model_hub,\n","      num_labels=len(label_list),\n","      learning_rate=LEARNING_RATE,\n","      num_train_steps=num_train_steps,\n","      num_warmup_steps=num_warmup_steps)\n","\n","    estimator = tf.estimator.Estimator(\n","      model_fn=model_fn,\n","      config=run_config,\n","      params={\"batch_size\": BATCH_SIZE})\n","    return estimator, model_fn, run_config"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S1W9SS8G0Zbd","colab_type":"code","colab":{}},"source":["def run_on_dfs(train, test, DATA_COLUMN, LABEL_COLUMN, \n","               MAX_SEQ_LENGTH = 400,\n","              BATCH_SIZE = 15,\n","              LEARNING_RATE = 2e-5,\n","              NUM_TRAIN_EPOCHS = 3.0,\n","              WARMUP_PROPORTION = 0.1,\n","              SAVE_SUMMARY_STEPS = 100,\n","              SAVE_CHECKPOINTS_STEPS = 10000,\n","              bert_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"):\n","\n","    label_list = train[LABEL_COLUMN].unique().tolist()\n","    \n","    tokenizer = create_tokenizer_from_hub_module(bert_model_hub)\n","\n","    train_features = make_features(train, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN)\n","    test_features = make_features(test, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN, LABEL_COLUMN)\n","\n","    num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","    estimator, model_fn, run_config = estimator_builder(\n","                                  bert_model_hub, \n","                                  OUTPUT_DIR, \n","                                  SAVE_SUMMARY_STEPS, \n","                                  SAVE_CHECKPOINTS_STEPS, \n","                                  label_list, \n","                                  LEARNING_RATE, \n","                                  num_train_steps, \n","                                  num_warmup_steps, \n","                                  BATCH_SIZE)\n","\n","    train_input_fn = bert.run_classifier.input_fn_builder(\n","        features=train_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=True,\n","        drop_remainder=False)\n","\n","    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","\n","    test_input_fn = run_classifier.input_fn_builder(\n","        features=test_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=False,\n","        drop_remainder=False)\n","\n","    result_dict = estimator.evaluate(input_fn=test_input_fn, steps=None)\n","    return result_dict, estimator"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"McY239_e0bXy","colab_type":"code","colab":{}},"source":["import random\n","random.seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tv2QTnXM0eXJ","colab_type":"code","colab":{}},"source":["OUTPUT_DIR = 'output'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQoEzO-1eML0","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\n","import random\n","\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import roc_auc_score, average_precision_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import average_precision_score, roc_auc_score\n","from sklearn.model_selection import StratifiedKFold"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCeSyw_eCrx4","colab_type":"code","outputId":"9acf482d-6b20-4a94-c91c-9a8d599eb652","executionInfo":{"status":"ok","timestamp":1567147980319,"user_tz":-120,"elapsed":20257,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCeI0QdxlRLcqMztd5HySSXx9D_ct4tg31n5g9W=s64","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# GOOGLE COLAB SETUP\n","\n","# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nak1uhxiEK0R","colab_type":"code","colab":{}},"source":["#2. Get the file\n","data_path     = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/data'\n","codes_path    = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/codes/separate_models'\n","cv_models_path     = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/cv_models'\n","models_path        = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/models'\n","OUTPUT_DIR = 'drive/My Drive/Colab Notebooks/adaptHAN/AOBDL_code/codes/separate_models/bert_output'\n","\n","\n","#3. Read file as panda dataframe\n","train         = pd.read_csv(f'{data_path}/train_cleaned_no_punkt.csv') \n","test_labelled = pd.read_csv(f'{data_path}/test_labelled_cleaned_no_punkt.csv') \n","test_unlabelled = pd.read_csv(f'{data_path}/test_unlabelled_cleaned_no_punkt.csv') "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f3ZeAcM_eGpj","colab_type":"code","colab":{}},"source":["train['mal'] = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1  \n","train.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n","train.comment_text.fillna(\"empty\", inplace=True)\n","\n","test_labelled['mal'] = test_labelled[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1  \n","test_labelled.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n","test_labelled.comment_text.fillna(\"empty\", inplace=True)\n","\n","test_unlabelled.comment_text.fillna(\"empty\", inplace=True)\n","\n","# CHANGE TRAIN AND TEST, MIX TO GET SIMILAR DISTRIBUTION\n","from sklearn.model_selection import train_test_split\n","rs=42\n","X_train1, X_test1, y_train1, y_test1  = train_test_split(train.drop('mal', axis=1), train.mal, stratify=train.mal, test_size=0.29, random_state=rs )\n","X_train2, X_test2, y_train2, y_test2  = train_test_split(test_labelled.drop('mal', axis=1), test_labelled.mal, stratify=test_labelled.mal, test_size=0.29, random_state=rs)\n","\n","X = np.concatenate((X_train1.comment_text, X_train2.comment_text))\n","y = np.concatenate((y_train1, y_train2))\n","\n","X_test = np.concatenate((X_test1.comment_text, X_test2.comment_text))\n","y_test = np.concatenate((y_test1, y_test2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWPSaXneFFZu","colab_type":"code","colab":{}},"source":["max_features = 40000\n","maxlen = 400\n","dropout_rate = 0\n","rs = 42\n","epochs = 4\n","batch_size = 250\n","embed_dim = 50\n","rec_units = 150"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BFgVXjNPF-dM","colab_type":"code","colab":{}},"source":["kf = StratifiedKFold(n_splits=5, random_state=rs)\n","auc = []\n","roc = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Df27savd041V","colab_type":"code","colab":{}},"source":["myparam = {\n","        \"DATA_COLUMN\": \"text\",\n","        \"LABEL_COLUMN\": \"mal\",\n","        \"LEARNING_RATE\": 2e-5,\n","        \"NUM_TRAIN_EPOCHS\":4,\n","        \"MAX_SEQ_LENGTH\" : 10\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-49hvW4_GHww","colab_type":"code","outputId":"e032edd7-0861-49b0-f8a3-b802aa75e5fb","executionInfo":{"status":"ok","timestamp":1567153909690,"user_tz":-120,"elapsed":3923595,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCeI0QdxlRLcqMztd5HySSXx9D_ct4tg31n5g9W=s64","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":598}},"source":["for c,(train_index, val_index) in enumerate(kf.split(X, y)):\n","    print(f' fold {c}')\n","    X_train, X_val       = X[train_index], X[val_index]\n","    y_train, y_val       = y[train_index], y[val_index] \n","    \n","    train_df = pd.DataFrame({'text': X_train, 'mal': y_train})\n","    val_df = pd.DataFrame({'text': X_val, 'mal': y_val})\n","   \n","    print('Fitting')\n","    \n","    result, estimator = run_on_dfs(train_df, val_df, **myparam)\n","    \n","    pretty_print(result)\n","    \n","   \n","    \n","#     probs                = model.predict(X_val, batch_size=batch_size, verbose=1)\n","#     auc_f                = average_precision_score(y_val, probs)\n","#     auc.append(auc_f)\n","#     roc_f                = roc_auc_score(y_val, probs)\n","#     roc.append(roc_f)\n","#     print(f' average precision {auc_f}')\n","#     print(f' roc auc {roc_f}')\n","#     del model\n","#     K.clear_session()"],"execution_count":18,"outputs":[{"output_type":"stream","text":[" fold 0\n","Fitting\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","W0830 07:28:48.868692 140180833314688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","W0830 07:28:51.544759 140180833314688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":[" fold 1\n","Fitting\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":[" fold 2\n","Fitting\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","W0830 08:24:54.416529 140180833314688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":[" fold 3\n","Fitting\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":[" fold 4\n","Fitting\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oD-G2cO9GLz3","colab_type":"code","colab":{}},"source":["data = pd.DataFrame({'acc':history.history['acc'],\n","                    'loss': history.history['loss'],\n","                    'val_acc': history.history['val_acc'],\n","                    'val_loss': history.history['val_loss']})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tEsMSyVxMs0j","colab_type":"code","outputId":"5b791ad9-b6ef-4d5c-8074-22b7b322f3a3","executionInfo":{"status":"ok","timestamp":1563188808705,"user_tz":-120,"elapsed":1268720,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["data"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>acc</th>\n","      <th>loss</th>\n","      <th>val_acc</th>\n","      <th>val_loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.935846</td>\n","      <td>0.184624</td>\n","      <td>0.940144</td>\n","      <td>0.146605</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.957370</td>\n","      <td>0.110126</td>\n","      <td>0.938223</td>\n","      <td>0.142197</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.962158</td>\n","      <td>0.099712</td>\n","      <td>0.926566</td>\n","      <td>0.164693</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.963403</td>\n","      <td>0.095061</td>\n","      <td>0.926314</td>\n","      <td>0.171205</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        acc      loss   val_acc  val_loss\n","0  0.935846  0.184624  0.940144  0.146605\n","1  0.957370  0.110126  0.938223  0.142197\n","2  0.962158  0.099712  0.926566  0.164693\n","3  0.963403  0.095061  0.926314  0.171205"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"S51--4EBjMKx","colab_type":"code","outputId":"2b3cf6d6-1ed3-4b2b-c016-a90459b97fd2","executionInfo":{"status":"ok","timestamp":1563188808707,"user_tz":-120,"elapsed":1268468,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["np.array(auc).mean()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8496062072656478"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"EPtxc1qPTfoC","colab_type":"code","outputId":"89d923c5-1791-4e40-fdbc-19691a30db72","executionInfo":{"status":"ok","timestamp":1563188808707,"user_tz":-120,"elapsed":1268061,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["np.array(roc).mean()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9677263515650806"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"r0hIZHoGctiz","colab_type":"code","outputId":"9e4c0386-984d-48f1-cff9-396ed4d7deaf","executionInfo":{"status":"ok","timestamp":1563189104850,"user_tz":-120,"elapsed":1544949,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["X_train = X\n","y_train = y\n","\n","tokenizer            = text.Tokenizer(num_words=max_features, oov_token='unknown')\n","tokenizer.fit_on_texts(X_train)\n","\n","list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n","list_tokenized_test  = tokenizer.texts_to_sequences(X_test)\n","X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n","X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n","\n","model = gru_keras(max_features, maxlen, dropout_rate, embed_dim, rec_units)\n","\n","y_train              = np.array(y_train)\n","y_test               = np.array(y_test)\n","\n","print('Fitting')\n","model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=True, verbose=1)\n","probs = model.predict(X_test, batch_size=batch_size, verbose=1)\n","auc_f = average_precision_score(y_test, probs)\n","roc_f = roc_auc_score(y_test, probs)\n","model.save_weights(f'{models_path}/BLSTM.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Fitting\n","Epoch 1/4\n","158719/158719 [==============================] - 68s 427us/step - loss: 0.1717 - acc: 0.9362\n","Epoch 2/4\n","158719/158719 [==============================] - 67s 420us/step - loss: 0.1152 - acc: 0.9556\n","Epoch 3/4\n","158719/158719 [==============================] - 67s 421us/step - loss: 0.1088 - acc: 0.9573\n","Epoch 4/4\n","158719/158719 [==============================] - 67s 421us/step - loss: 0.1048 - acc: 0.9590\n","64830/64830 [==============================] - 9s 146us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sjoDc3zpDImG","colab_type":"code","outputId":"c36fa95e-d5d0-4fc6-c172-782b3774ceeb","executionInfo":{"status":"ok","timestamp":1563189104853,"user_tz":-120,"elapsed":1544352,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["auc_f"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8519709554707031"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"k-ptNl8KDarn","colab_type":"code","outputId":"e1c84a93-9c4f-49ca-f6db-ddd412c4a5ff","executionInfo":{"status":"ok","timestamp":1563189104855,"user_tz":-120,"elapsed":1543891,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["roc_f"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9711581639591236"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"Tir9BE9Ul5Ps","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}