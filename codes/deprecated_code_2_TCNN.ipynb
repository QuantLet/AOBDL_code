{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code_2_TCNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"pnWPIq0NYYr_","colab_type":"code","outputId":"b6bf9ade-7505-444b-c8a7-e894f9ebc76f","executionInfo":{"status":"ok","timestamp":1559727536299,"user_tz":-120,"elapsed":1240,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pandas as pd\n","from keras.optimizers import RMSprop\n","import tensorflow as tf"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"1f2mr26SZNfl","colab_type":"code","outputId":"9e143d31-4678-4995-e100-ae3266e76c92","executionInfo":{"status":"ok","timestamp":1559727577996,"user_tz":-120,"elapsed":42239,"user":{"displayName":"Elizaveta","photoUrl":"https://lh3.googleusercontent.com/-3-3kSLC4Mzw/AAAAAAAAAAI/AAAAAAAAABg/Usb9K3n3cRI/s64/photo.jpg","userId":"01253993997636551956"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6S_vBLLzZr_9","colab_type":"code","colab":{}},"source":["data_path     = 'drive/My Drive/Colab Notebooks/adaptHAN_TCNN/data'\n","codes_path    = 'drive/My Drive/Colab Notebooks/adaptHAN_TCNN/codes'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pLH2l7sZSCI","colab_type":"code","colab":{}},"source":["train           = pd.read_csv(f'{data_path}/train_cleaned_no_punkt.csv')\n","test_labelled   = pd.read_csv(f'{data_path}/test_labelled_cleaned_no_punkt.csv')\n","test_unlabelled = pd.read_csv(f'{data_path}/test_unlabelled_cleaned_no_punkt.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8XZ0izIyZsZ0","colab_type":"code","colab":{}},"source":["train['mal'] = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1  \n","train.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n","train.comment_text.fillna(\"empty\", inplace=True)\n","\n","test_labelled['mal'] = test_labelled[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 1  \n","test_labelled.drop(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1, inplace=True)\n","test_labelled.comment_text.fillna(\"empty\", inplace=True)\n","\n","test_unlabelled.comment_text.fillna(\"empty\", inplace=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l3Ew6DqiZ4I7","colab_type":"code","colab":{}},"source":["train['len_words'] = train.comment_text.apply(lambda x: len(x.split()))\n","train['len_chars'] = train.comment_text.apply(lambda x: len(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKyQhB14avKy","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","from keras.models import Model\n","from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n","from keras.layers import CuDNNGRU, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n","from keras.preprocessing import text, sequence\n","from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n","from keras.losses import binary_crossentropy\n","from keras import backend as K\n","from tqdm import tqdm_notebook as tqdm\n","import pickle\n","import gc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3VYLh3rczuB","colab_type":"code","colab":{}},"source":["# from https://github.com/philipperemy/keras-tcn\n","\n","\n","import keras.backend as K\n","import keras.layers\n","from keras import optimizers\n","from keras.engine.topology import Layer\n","from keras.layers import Activation, Lambda\n","from keras.layers import Conv1D, SpatialDropout1D\n","from keras.layers import Convolution1D, Dense\n","from keras.models import Input, Model\n","from typing import List, Tuple\n","\n","\n","def channel_normalization(x):\n","    # type: (Layer) -> Layer\n","    \"\"\" Normalize a layer to the maximum activation\n","    This keeps a layers values between zero and one.\n","    It helps with relu's unbounded activation\n","    Args:\n","        x: The layer to normalize\n","    Returns:\n","        A maximal normalized layer\n","    \"\"\"\n","    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n","    out = x / max_values\n","    return out\n","\n","\n","def wave_net_activation(x):\n","    # type: (Layer) -> Layer\n","    \"\"\"This method defines the activation used for WaveNet\n","    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n","    Args:\n","        x: The layer we want to apply the activation to\n","    Returns:\n","        A new layer with the wavenet activation applied\n","    \"\"\"\n","    tanh_out = Activation('tanh')(x)\n","    sigm_out = Activation('sigmoid')(x)\n","    return keras.layers.multiply([tanh_out, sigm_out])\n","\n","\n","def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n","    # type: (Layer, int, int, str, int, int, float, str) -> Tuple[Layer, Layer]\n","    \"\"\"Defines the residual block for the WaveNet TCN\n","    Args:\n","        x: The previous layer in the model\n","        s: The stack index i.e. which stack in the overall TCN\n","        i: The dilation power of 2 we are using for this residual block\n","        activation: The name of the type of activation to use\n","        nb_filters: The number of convolutional filters to use in this block\n","        kernel_size: The size of the convolutional kernel\n","        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n","        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n","        name: Name of the model. Useful when having multiple TCN.\n","    Returns:\n","        A tuple where the first element is the residual model layer, and the second\n","        is the skip connection.\n","    \"\"\"\n","\n","    original_x = x\n","    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n","                  dilation_rate=i, padding=padding,\n","                  name=name + '_dilated_conv_%d_tanh_s%d' % (i, s))(x)\n","    if activation == 'norm_relu':\n","        x = Activation('relu')(conv)\n","        x = Lambda(channel_normalization)(x)\n","    elif activation == 'wavenet':\n","        x = wave_net_activation(conv)\n","    else:\n","        x = Activation(activation)(conv)\n","\n","    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n","\n","    # 1x1 conv.\n","    x = Convolution1D(nb_filters, 1, padding='same')(x)\n","    res_x = keras.layers.add([original_x, x])\n","    return res_x, x\n","\n","\n","def process_dilations(dilations):\n","    def is_power_of_two(num):\n","        return num != 0 and ((num & (num - 1)) == 0)\n","\n","    if all([is_power_of_two(i) for i in dilations]):\n","        return dilations\n","\n","    else:\n","        new_dilations = [2 ** i for i in dilations]\n","        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n","        return new_dilations\n","\n","\n","class TCN(Layer):\n","    \"\"\"Creates a TCN layer.\n","        Args:\n","            input_layer: A tensor of shape (batch_size, timesteps, input_dim).\n","            nb_filters: The number of filters to use in the convolutional layers.\n","            kernel_size: The size of the kernel to use in each convolutional layer.\n","            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n","            nb_stacks : The number of stacks of residual blocks to use.\n","            activation: The activations to use (norm_relu, wavenet, relu...).\n","            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n","            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n","            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n","            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n","            name: Name of the model. Useful when having multiple TCN.\n","        Returns:\n","            A TCN layer.\n","        \"\"\"\n","\n","    def __init__(self,\n","                 nb_filters=64,\n","                 kernel_size=2,\n","                 nb_stacks=1,\n","                 dilations=None,\n","                 activation='norm_relu',\n","                 padding='causal',\n","                 use_skip_connections=True,\n","                 dropout_rate=0.0,\n","                 return_sequences=True,\n","                 name='tcn'):\n","        super().__init__()\n","        self.name = name\n","        self.return_sequences = return_sequences\n","        self.dropout_rate = dropout_rate\n","        self.use_skip_connections = use_skip_connections\n","        self.activation = activation\n","        self.dilations = dilations\n","        self.nb_stacks = nb_stacks\n","        self.kernel_size = kernel_size\n","        self.nb_filters = nb_filters\n","        self.padding = padding\n","\n","        # backwards incompatibility warning.\n","        # o = tcn.TCN(i, return_sequences=False) =>\n","        # o = tcn.TCN(return_sequences=False)(i)\n","\n","        if padding != 'causal' and padding != 'same':\n","            raise ValueError(\"Only 'causal' or 'same' paddings are compatible for this layer.\")\n","\n","        if not isinstance(nb_filters, int):\n","            print('An interface change occurred after the version 2.1.2.')\n","            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n","            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n","            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n","            raise Exception()\n","\n","    def __call__(self, inputs):\n","        if self.dilations is None:\n","            self.dilations = [1, 2, 4, 8, 16, 32]\n","        x = inputs\n","        #x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n","        x = Dense(self.nb_filters, name=self.name + '_initial_conv')(x)\n","        skip_connections = []\n","        for s in range(self.nb_stacks):\n","            for i in self.dilations:\n","                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n","                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n","                skip_connections.append(skip_out)\n","        if self.use_skip_connections:\n","            x = keras.layers.add(skip_connections)\n","        x = Activation('relu')(x)\n","\n","        if not self.return_sequences:\n","            output_slice_index = -1\n","            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzbzJ2l5dL3S","colab_type":"code","colab":{}},"source":["# CHANGE TRAIN AND TEST, MIX TO GET SIMILAR DISTRIBUTION\n","from sklearn.model_selection import train_test_split\n","rs = 43\n","X_train1, X_test1, y_train1, y_test1  = train_test_split(train.drop('mal', axis=1), train.mal, stratify=train.mal, test_size=0.29, random_state=rs )\n","X_train2, X_test2, y_train2, y_test2  = train_test_split(test_labelled.drop('mal', axis=1), test_labelled.mal, stratify=test_labelled.mal, test_size=0.29, random_state=rs)\n","\n","X = np.concatenate((X_train1.comment_text, X_train2.comment_text))\n","y = np.concatenate((y_train1, y_train2))\n","\n","X_test = np.concatenate((X_test1.comment_text, X_test2.comment_text))\n","y_test = np.concatenate((y_test1, y_test2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rA6Aa6PedPsK","colab_type":"code","colab":{}},"source":["max_features       = 50000\n","maxlen             = 400\n","dropout_rate       = 0.25\n","rs                 = 42\n","epochs             = 8\n","batch_size         = 128\n","embed_dim          = 50\n","rec_units          = 150\n","TCN_UNITS          = 75\n","DENSE_HIDDEN_UNITS = TCN_UNITS*2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"arH1Z-DDc58i","colab_type":"code","colab":{}},"source":["def make_hat(maxlen=maxlen, embed_dim=embed_dim, max_features=max_features, dropout_rate=dropout_rate, \n","            DENSE_HIDDEN_UNITS=DENSE_HIDDEN_UNITS, TCN_UNITS=TCN_UNITS):\n","    words = Input(shape=(maxlen,))\n","    x = Embedding(max_features+1, embed_dim, trainable=True)(words)\n","    x = SpatialDropout1D(dropout_rate)(x)\n","    x1 = TCN(TCN_UNITS, return_sequences=True, dilations = [1, 2, 4, 8, 16], name = 'tnc1_forward')(x) #, activation = 'wavenet'\n","    x2 = Lambda(lambda z: K.reverse(z,axes=-1))(x)\n","    x2 = TCN(TCN_UNITS, return_sequences=True, dilations = [1, 2, 4, 8, 16],name = 'tnc1_backward')(x2) #,dilations = [1, 2, 4]\n","    x = add([x1,x2])\n","    x1 = TCN(TCN_UNITS, return_sequences=True, dilations = [1, 2, 4, 8, 16], name = 'tnc2_forward')(x)\n","    x2 = Lambda(lambda z: K.reverse(z,axes=-1))(x)\n","    x2 = TCN(TCN_UNITS, return_sequences=True, dilations = [1, 2, 4, 8, 16],name = 'tnc2_backward')(x2)\n","    x = add([x1,x2])\n","    #x = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x)])\n","    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x)])\n","    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n","    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n","    result = Dense(1, activation='sigmoid')(hidden)\n","\n","\n","    model = Model(inputs=words, outputs=result)\n","    model.compile(loss='binary_crossentropy', optimizer=RMSprop(clipvalue=1, clipnorm=1), metrics=['acc'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zCjvc4Wkf-cB","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import StratifiedKFold\n","kf = StratifiedKFold(n_splits=5, random_state=rs)\n","auc = []\n","roc = []\n","c = 0\n","tokenizer = text.Tokenizer(num_words=max_features)\n","tokenizer.fit_on_texts(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5P9VvE00C2i_","colab_type":"code","colab":{}},"source":["from keras.preprocessing import text, sequence\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.optimizers import RMSprop, Adam\n","from keras.engine.topology import Layer, InputSpec\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import StratifiedKFold\n","from nltk import tokenize \n","\n","from keras.models import Model\n","from keras import backend as K\n","\n","from keras.engine.topology import Layer, InputSpec\n","from keras import initializers as initializers, regularizers, constraints\n","from keras.callbacks import Callback\n","\n","class AucPrRecEvaluation(Callback):\n","    def __init__(self, validation_data=(), interval=1):\n","        super(Callback, self).__init__()\n","\n","        self.interval = interval\n","        self.X_val, self.y_val = validation_data\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        if epoch % self.interval == 0:\n","            y_pred = self.model.predict(self.X_val, verbose=0)\n","            score = average_precision_score(self.y_val, y_pred)\n","            print(\"\\n AUC-Precision Recall - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L84xHiFYeyMY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":714},"outputId":"c365eb1c-3df2-4af2-92a6-a3897d928a8f"},"source":["from sklearn.metrics import average_precision_score, roc_auc_score\n","from keras.preprocessing.sequence import pad_sequences\n","\n","for train_index, val_index in kf.split(X, y):\n","    print(f' fold {c}')\n","    X_train, X_val = X[train_index], X[val_index]\n","    y_train, y_val = y[train_index], y[val_index] \n","    \n","    X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=maxlen, padding='post')\n","    X_val = pad_sequences(tokenizer.texts_to_sequences(X_val), maxlen=maxlen, padding='post')\n","    #exipdb.set_trace()\n","    #X_train = np.array([line.reshape(max_sent_amount,max_sen_len) for line in X_train])\n","    #X_val = np.array([line.reshape(max_sent_amount,max_sen_len) for line in X_val])\n","    \n","    PrAuc = AucPrRecEvaluation(validation_data=(X_val, y_val), interval=1)\n","    callbacks_list = [PrAuc]\n","    \n","    model = make_hat()\n","    #print(model.summary())\n","    # model = tf.contrib.tpu.keras_to_tpu_model(\n","    #          model,\n","    #          strategy=tf.contrib.tpu.TPUDistributionStrategy(\n","    #              tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n","    #break\n","    \n","    #X_train = np.array(train_posts)\n","    y_train = np.array(y_train)\n","    #X_val =  np.array(val_posts)\n","    y_val = np.array(y_val)\n","   \n","    print('Fitting')\n","    model.fit(X_train, y_train, batch_size=batch_size, epochs=4, validation_data=(X_val, y_val), callbacks=callbacks_list, shuffle=False, verbose=1)\n","    probs = model.predict(X_val, batch_size=batch_size, verbose=1)\n","    \n","    average_avpr = []\n","    average_auc = []\n","    \n","    \n","    auc_f = average_precision_score(y_val, probs)\n","    auc.append(auc_f)\n","    roc_f = roc_auc_score(y_val, probs)\n","    roc.append(roc_f)\n","    print(f' average precision {auc_f}')\n","    print(f' roc auc {roc_f}')\n","    c += 1\n","    del model"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" fold 0\n","Fitting\n","Train on 126974 samples, validate on 31745 samples\n","Epoch 1/4\n","126974/126974 [==============================] - 226s 2ms/step - loss: 0.2379 - acc: 0.9331 - val_loss: 0.1464 - val_acc: 0.9440\n","\n"," AUC-Precision Recall - epoch: 1 - score: 0.842843 \n","\n","Epoch 2/4\n","126974/126974 [==============================] - 225s 2ms/step - loss: 0.1211 - acc: 0.9548 - val_loss: 0.1451 - val_acc: 0.9483\n","\n"," AUC-Precision Recall - epoch: 2 - score: 0.855718 \n","\n","Epoch 3/4\n","126974/126974 [==============================] - 226s 2ms/step - loss: 0.1142 - acc: 0.9572 - val_loss: 0.1411 - val_acc: 0.9499\n","\n"," AUC-Precision Recall - epoch: 3 - score: 0.856684 \n","\n","Epoch 4/4\n","126974/126974 [==============================] - 226s 2ms/step - loss: 0.1098 - acc: 0.9585 - val_loss: 0.1440 - val_acc: 0.9513\n","\n"," AUC-Precision Recall - epoch: 4 - score: 0.856008 \n","\n","31745/31745 [==============================] - 16s 498us/step\n"," average precision 0.8560081496917137\n"," roc auc 0.963142978671079\n"," fold 1\n","Fitting\n","Train on 126974 samples, validate on 31745 samples\n","Epoch 1/4\n","126974/126974 [==============================] - 227s 2ms/step - loss: 0.1828 - acc: 0.9365 - val_loss: 0.1407 - val_acc: 0.9494\n","\n"," AUC-Precision Recall - epoch: 1 - score: 0.852918 \n","\n","Epoch 2/4\n","126974/126974 [==============================] - 224s 2ms/step - loss: 0.1241 - acc: 0.9546 - val_loss: 0.1436 - val_acc: 0.9499\n","\n"," AUC-Precision Recall - epoch: 2 - score: 0.847828 \n","\n","Epoch 3/4\n","  4480/126974 [>.............................] - ETA: 3:20 - loss: 0.1220 - acc: 0.9589"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kBfiAL1VEYrF","colab_type":"code","colab":{}},"source":["np.array(auc).mean()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKOASI0A7Cd1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}